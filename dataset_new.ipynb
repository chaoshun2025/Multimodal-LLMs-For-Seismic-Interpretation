{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|████████████████████████████████████████████████████████████████| 51/51 [00:00<00:00, 87.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Features, Value\n",
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# load images and save to bytes\n",
    "def load_image_as_bytes(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    # 调整图像大小（如果需要）\n",
    "    image = image.resize((512, 512))  # 确保图像大小一致\n",
    "    image_array = np.array(image)\n",
    "    # 将 NumPy 数组转换为字节流\n",
    "    return image_array.tobytes()\n",
    "\n",
    "# load json file to create dataset\n",
    "def load_custom_dataset(json_file_path, image_folder_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    dataset_data = {\n",
    "        \"image_name\": [],\n",
    "        \"caption\": [],\n",
    "        \"image\": [],\n",
    "    }\n",
    "    \n",
    "    # use tqdm to show loading progress\n",
    "    for idx in tqdm(range(len(data[\"annotations\"])), desc=\"Processing dataset\"):\n",
    "        annotation = data[\"annotations\"][idx]\n",
    "        image_path = os.path.join(image_folder_path, annotation[\"filepath\"])\n",
    "        \n",
    "        dataset_data[\"image_name\"].append(annotation[\"filepath\"])\n",
    "        dataset_data[\"caption\"].append(annotation[\"caption\"])\n",
    "        dataset_data[\"image\"].append(load_image_as_bytes(image_path))\n",
    "    \n",
    "    # define the features of the dataset\n",
    "    features = Features({\n",
    "        \"image_name\": Value(\"string\"),\n",
    "        \"caption\": Value(\"string\"),\n",
    "        \"image\": Value(\"binary\")  # save to bytes stream\n",
    "    })\n",
    "    \n",
    "    # create dataset\n",
    "    dataset = Dataset.from_dict(dataset_data, features=features)\n",
    "    return dataset\n",
    "\n",
    "# convert Dataset to Parquet file\n",
    "def dataset_to_parquet(dataset, output_path):\n",
    "    # convert Dataset to Pandas DataFrame\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    # create PyArrow Table\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    \n",
    "    # save to Parquet file\n",
    "    pq.write_table(table, output_path)\n",
    "\n",
    "# call to load dataset\n",
    "json_file_path = \"./DataPrep/data.json\"  # JSON filepath\n",
    "image_folder_path = \"./DataPrep/images/\"  # image filepath\n",
    "dataset = load_custom_dataset(json_file_path, image_folder_path)\n",
    "\n",
    "# dump to Parquet file\n",
    "output_parquet_path = \"./dataset.parquet\"\n",
    "dataset_to_parquet(dataset, output_parquet_path)\n",
    "print(f\"Dataset saved to {output_parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         image_name                                            caption  \\\n",
      "0  img1_seismic.jpg  This seismic reflection image shows undeformed...   \n",
      "1  img2_seismic.jpg  This seismic reflection image features a clear...   \n",
      "2  img3_seismic.jpg  this seismic reflection image displays a layer...   \n",
      "3  img4_seismic.jpg  This image shows a seismic reflection profile ...   \n",
      "4  img5_seismic.jpg  The image appears to be a seismic reflection p...   \n",
      "\n",
      "                                               image  \n",
      "0  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...  \n",
      "1  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...  \n",
      "2  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...  \n",
      "3  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...  \n",
      "4  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...  \n"
     ]
    }
   ],
   "source": [
    "# load the saved Parquet file\n",
    "loaded_df = pd.read_parquet(\"dataset.parquet\")\n",
    "\n",
    "# check the 5th records\n",
    "print(loaded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401549c156b541b38932f8b532d83274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec8ac4667c349d3802d72b7b7f733ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f70aa3091e43b99ff585a083f5f206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image conversion successful.\n",
      "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512 at 0x155E4C550>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image_name': 'img1_seismic.jpg',\n",
       " 'caption': 'This seismic reflection image shows undeformed, continuous, horizontal strata, characteristic of a structurally simple sedimentary sequence. The uniform layering and lack of disruptions suggest minimal tectonic activity, making it a likely representation of a stable depositional environment such as a marine shelf or deep-water basin.',\n",
       " 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Features, Value, Image\n",
    "import numpy as np\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "# load Parquet file\n",
    "def load_parquet_dataset(file_path):\n",
    "    dataset = load_dataset(\"parquet\", data_files=file_path)\n",
    "    return dataset\n",
    "\n",
    "# restore bytes to image\n",
    "def bytes_to_image(byte_data, shape=(512, 512, 3), dtype=np.uint8):\n",
    "    try:\n",
    "        # convert bytes to NumPy array\n",
    "        image_array = np.frombuffer(byte_data, dtype=dtype)\n",
    "        # reshape to image\n",
    "        image_array = image_array.reshape(shape)\n",
    "        # create the PILImage\n",
    "        image = PILImage.fromarray(image_array)\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting byte data to image: {e}\")\n",
    "        return None\n",
    "\n",
    "parquet_file_path = \"./dataset.parquet\"\n",
    "dataset = load_parquet_dataset(parquet_file_path)\n",
    "\n",
    "# use map to convert bytes to image\n",
    "try:\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\"image\": bytes_to_image(x[\"image\"])},\n",
    "        batched=False,\n",
    "        num_proc=6  # use 6 processes\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error during map operation: {e}\")\n",
    "\n",
    "# redefine the features\n",
    "features = Features({\n",
    "    \"image_name\": Value(\"string\"),\n",
    "    \"caption\": Value(\"string\"),\n",
    "    \"image\": Image()  \n",
    "})\n",
    "\n",
    "# convert dataset to new features\n",
    "dataset = dataset.cast(features)\n",
    "\n",
    "# QC\n",
    "for sample in dataset[\"train\"]:\n",
    "    if isinstance(sample[\"image\"], PILImage.Image):\n",
    "        print(\"Image conversion successful.\")\n",
    "        print(sample[\"image\"])  # print image sample\n",
    "        break\n",
    "    else:\n",
    "        print(\"Image conversion failed.\")\n",
    "        break\n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_name': 'P2691_0130.png',\n",
       " 'caption': 'This is an aerial image of a parking lot. In the bottom right corner of the image, there is a parking lot with many parked cars. There is a white building near the parking lot. In the top left corner of the image, there is a piece of wasteland. A road runs through the wasteland.',\n",
       " 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# split dataset per 80%, 10%, 10% ratio\n",
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "\n",
    "validation_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "validation_dataset = validation_test_split[\"train\"]\n",
    "test_dataset = validation_test_split[\"test\"]\n",
    "\n",
    "# construct a new DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "dataset_dict[\"train\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
