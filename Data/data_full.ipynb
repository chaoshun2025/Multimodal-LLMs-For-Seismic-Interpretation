{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/multillama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image_path', 'caption'],\n",
      "    num_rows: 2585\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Features, Value, Sequence,DatasetDict,load_dataset\n",
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# definition of features\n",
    "features = Features({\n",
    "    \"image_path\": Value(\"string\"),\n",
    "    \"caption\": Value(\"string\"),\n",
    "})\n",
    "\n",
    "# load json file to create dataset\n",
    "def load_custom_dataset(json_file_path, image_folder_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    dataset_data = {\n",
    "        \"image_path\": [],\n",
    "        \"caption\": [],\n",
    "    }\n",
    "    for annotation in data[\"annotations\"]:\n",
    "        image_path = os.path.join(image_folder_path, annotation[\"filename\"])\n",
    "        \n",
    "        dataset_data[\"image_path\"].append(image_path)\n",
    "        dataset_data[\"caption\"].append(annotation[\"caption\"])\n",
    "    \n",
    "    # create dataset\n",
    "    dataset = Dataset.from_dict(dataset_data, features=features)\n",
    "    return dataset\n",
    "\n",
    "# call\n",
    "json_file_path = \"./data/captions.json\"  # JSON filepath\n",
    "image_folder_path = \"./data/images/\"  # image filepath\n",
    "dataset = load_custom_dataset(json_file_path, image_folder_path)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 2068\n",
      "Validation dataset size: 258\n",
      "Test dataset size: 259\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image_path', 'caption'],\n",
      "        num_rows: 2068\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image_path', 'caption'],\n",
      "        num_rows: 258\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image_path', 'caption'],\n",
      "        num_rows: 259\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#  split dataset and use 80% as training dataset\n",
    "dataset_split = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# further split for validation and testing as 10% and 10%\n",
    "test_valid_split = dataset_split['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# finalized the training, validate and test dataset\n",
    "train_dataset = dataset_split['train']\n",
    "valid_dataset = test_valid_split['train']\n",
    "test_dataset = test_valid_split['test']\n",
    "\n",
    "# create a DatasetDict object\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "\n",
    "# QC the size of each dataset\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable to save train, validation and test dataset\n",
    "train_path = \"./data/train.json\"\n",
    "validation_path = \"./data/validation.json\"\n",
    "test_path = \"./data/test.json\"\n",
    "\n",
    "# save train, valid and test dataset from dataset_dict \n",
    "def save_dataset_to_json(dataset_dict, train_path, validation_path, test_path):\n",
    "    # save train dataset\n",
    "    with open(train_path, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset_dict['train']:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # save validation dataset\n",
    "    with open(validation_path, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset_dict['validation']:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # save testing dataset\n",
    "    with open(test_path, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset_dict['test']:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "# writeout\n",
    "save_dataset_to_json(dataset_dict, train_path, validation_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2068 examples [00:00, 279800.67 examples/s]\n",
      "Generating validation split: 258 examples [00:00, 50578.66 examples/s]\n",
      "Generating test split: 259 examples [00:00, 52632.01 examples/s]\n",
      "Map: 100%|██████████| 2068/2068 [03:33<00:00,  9.67 examples/s] \n",
      "Map: 100%|██████████| 258/258 [00:18<00:00, 14.17 examples/s] \n",
      "Map: 100%|██████████| 259/259 [00:17<00:00, 14.43 examples/s] \n",
      "Saving the dataset (2/2 shards): 100%|██████████| 2068/2068 [00:01<00:00, 1358.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 258/258 [00:00<00:00, 1954.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 259/259 [00:00<00:00, 1977.68 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': './data/RSICap/images/P2141_0007.png', 'caption': 'This is an aerial image that depicts an area near a football field. Specifically, the football field is located in the upper left corner of the image. Surrounding the football field, there is a grey-colored running track. In the upper right corner of the image, there is a patch of forest. Moving towards the bottom right corner of the image, there is an open area with three cars on it. In the upper left area of the open area, there is another patch of forest. And in the bottom left corner of the open area, there is a swimming pool.', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512 at 0x701E39327DD0>}\n",
      "{'image_path': './data/RSICap/images/P2141_0007.png', 'caption': 'This is an aerial image that depicts an area near a football field. Specifically, the football field is located in the upper left corner of the image. Surrounding the football field, there is a grey-colored running track. In the upper right corner of the image, there is a patch of forest. Moving towards the bottom right corner of the image, there is an open area with three cars on it. In the upper left area of the open area, there is another patch of forest. And in the bottom left corner of the open area, there is a swimming pool.', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512 at 0x701E39326B10>}\n",
      "{'image_path': './data/RSICap/images/P0896_0015.png', 'caption': 'This is an aerial image showing water bodies. In the top portion of the image, there are seven ships and three partially visible buildings. On the right side of the image, there is a partially visible road.', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512 at 0x701E393263D0>}\n",
      "{'image_path': './data/RSICap/images/P2257_0005.png', 'caption': 'This is an aerial image that shows a playground on the left side of the image with a soccer field located in the center of the playground. There is a grass area located below the playground. A brown bare land area surrounded by trees is located on the right side of the image.', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512 at 0x701E3930D690>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "# load the dataset\n",
    "dataset_path = \"./data\"\n",
    "train_dataset = load_dataset('json', data_files={'train': f'{dataset_path}/train.json'})['train']\n",
    "validation_dataset = load_dataset('json', data_files={'validation': f'{dataset_path}/validation.json'})['validation']\n",
    "test_dataset = load_dataset('json', data_files={'test': f'{dataset_path}/test.json'})['test']\n",
    "\n",
    "# define a mapping function to load image to example['image']\n",
    "def preprocess_example(example):\n",
    "    example['image'] = load_image(example['image_path'])\n",
    "    return example\n",
    "\n",
    "# apply mapping function to three datasets\n",
    "train_dataset = train_dataset.map(preprocess_example)\n",
    "validation_dataset = validation_dataset.map(preprocess_example)\n",
    "test_dataset = test_dataset.map(preprocess_example)\n",
    "\n",
    "# create DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "#dump to local disk\n",
    "dataset_dict.save_to_disk('./data/dataset_full/')\n",
    "\n",
    "# qc the 1st element\n",
    "print(dataset_dict['train'][0])\n",
    "\n",
    "#load from disk\n",
    "from datasets import load_from_disk\n",
    "train_dataset_new = load_from_disk('./data/dataset_full/train')\n",
    "validation_dataset_new = load_from_disk('./data/dataset_full/validation')\n",
    "test_dataset_new = load_from_disk('./data/dataset_full/test')\n",
    "\n",
    "print(train_dataset_new[0])\n",
    "print(validation_dataset_new[0])\n",
    "print(test_dataset_new[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
